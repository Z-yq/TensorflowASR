model_config:
  name: Transformer
  num_layers: 3
  d_model: 256
  enc_embedding_dim: 512
  dec_embedding_dim: 512
  num_heads: 8
  dff: 1024
  pe_input: 1024
  pe_target: 1024
  rate: 0.1
  one2one: False
  include_decoder: True
optimizer_config:
  learning_rate: 0.0001
  beta_1: 0.9
  beta_2: 0.98
  epsilon: 0.000001

